apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: local-ai
  namespace: argocd
spec:
  syncPolicy:
    automated: {}
    syncOptions:
    - CreateNamespace=true
  project: default
  sources:
    - chart: local-ai
      repoURL: https://go-skynet.github.io/helm-charts/
      targetRevision: 3.4.2
      helm:
        values: |
          replicaCount: 1

          deployment:
            image:
              repository: quay.io/go-skynet/local-ai
              tag: latest-cpu
            env:
              threads: 4
              context_size: 512

            # # Inject Secrets into Environment:
            # secretEnv:
            # - name: HF_TOKEN
            #   valueFrom:
            #     secretKeyRef:
            #       name: some-secret
            #       key: hf-token

            modelsPath: "/models"
            prompt_templates:
              # To use cloud provided (eg AWS) image, provide it like: 1234356789.dkr.ecr.us-REGION-X.amazonaws.com/busybox
              image: busybox
            pullPolicy: IfNotPresent
            imagePullSecrets: []
              # - name: secret-names

            ## Needed for GPU Nodes
            #runtimeClassName: gpu

          resources:
            {}
            # We usually recommend not to specify default resources and to leave this as a conscious
            # choice for the user. This also increases chances charts run on environments with little
            # resources, such as Minikube. If you do want to specify resources, uncomment the following
            # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
            # limits:
            #   cpu: 100m
            #   memory: 128Mi
            # requests:
            #   cpu: 100m
            #   memory: 128Mi

          # Model config to include
          modelsConfigs:
            {}
            # phi-2: |
            #   name: phi-2
            #   context_size: 2048
            #   f16: true
            #   mmap: true
            #   trimsuffix:
            #   - "\n"
            #   parameters:
            #     model: phi-2.Q8_0.gguf
            #     temperature: 0.2
            #     top_k: 40
            #     top_p: 0.95
            #     seed: -1
            #   template:
            #     chat: &template |-
            #       Instruct: {{.Input}}
            #       Output:
            #     completion: *template

          # Prompt templates to include
          # Note: the keys of this map will be the names of the prompt template files
          promptTemplates:
            {}
            # ggml-gpt4all-j.tmpl: |
            #   The prompt below is a question to answer, a task to complete, or a conversation to respond to; decide which and write an appropriate response.
            #   ### Prompt:
            #   {{.Input}}
            #   ### Response:

          initContainers: []
          # Example:
          # - name: my-init-container
          #   image: my-init-image
          #   imagePullPolicy: IfNotPresent
          #   command: ["/bin/sh", "-c", "echo init"]
          #   volumeMounts:
          #     - name: my-volume
          #       mountPath: /path/to/mount

          sidecarContainers: []
          # Example:
          # - name: my-sidecar-container
          #   image: my-sidecar-image
          #   imagePullPolicy: IfNotPresent
          #   ports:
          #     - containerPort: 1234

          # Persistent storage for models and prompt templates.
          # PVC and HostPath are mutually exclusive. If both are enabled,
          # PVC configuration takes precedence. If neither are enabled, ephemeral
          # storage is used.
          persistence:
            models:
              enabled: true
              annotations: {}
              storageClass: "ceph-filesystem" # Use default storage class
              accessModes:
                - ReadWriteMany
              size: 10Gi
              globalMount: /models
            output:
              enabled: true
              annotations: {}
              storageClass: "ceph-filesystem" # Use default storage class
              accessModes:
                - ReadWriteMany
              size: 5Gi
              globalMount: /tmp/generated
        valuesObject:
          # Models to download at runtime
          models:
            # Whether to force download models even if they already exist
            forceDownload: false

            # The list of URLs to download models from
            # Note: the name of the file will be the name of the loaded model
            list:
              - url: "https://gpt4all.io/models/ggml-gpt4all-j.bin"
            persistence:
              pvc:
                enabled: true
                size: 16Gi
                accessModes:
                  - ReadWriteMany

                annotations: {}

                # Optional
                storageClass: "ceph-filesystem"

              hostPath:
                enabled: false
                path: "/models"

  destination:
    server: https://kubernetes.default.svc
    namespace: local-ai
...